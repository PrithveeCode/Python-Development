{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Networking in Python**\n",
    "\n",
    ">**Que.** What is a Protocol ?\\\n",
    ">**Ans.** A protocol is a set of precise rules that determine who is to go first, what they are to do, and then what the responses are to that message, and who sends next, and so on. In a sense the two applications at either end of the socket are doing a dance and making sure not to step on each other toes.\n",
    "\n",
    ">**Que.** What is Socket -?\\\n",
    ">**Ans.** Sockets: 2 - Way Communication, Sockets - Most of The Time is A Blocking Wait until data is available to read!.\n",
    "\n",
    "\n",
    ">**Que.** What is TCP ?\\\n",
    ">**Ans.** Full-form: Transport Control Protocol. This Layer is built on Top of IP (**Internet Protocol**). If Some data is lost during IP communication. Data is repeated. (retransmit data, if lost.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1. TCP Port Numbers:**\n",
    "- A port is an Application-Specific or Process-Specific Software Communication Ends.\n",
    "- It allows multiple networked application to coexist on the same server.\n",
    "- There's Alsp a list of Well-Known TCP Port Number.\n",
    "\n",
    "-   | TCP Ports     | Functionality             |\n",
    "    | --------------| --------------------------|         \n",
    "    | 23            | Telnet(Login)             |\n",
    "    | 22            | SSH (Secure Login)        |\n",
    "    | 80            | HTTP                      |\n",
    "    | 443           | HTTPS                     |\n",
    "    | 25            | SMTP                      |\n",
    "    | 143/220/993   | IMAP                      |\n",
    "    | 109/110       | POP                       |\n",
    "    | 53            | DNS (Domain Name)         |\n",
    "    | 21            | FTP (File Transfer)       |\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2. HTTP - HyperText Transfer Protocol.**\n",
    "\n",
    "- **What Exactly is This ?** -> Set of Rules, For The Internet To Allow The Browser To Retreive Web Docs from Servers over The Internet.\n",
    "- Dominant Protocol in Application layer in The Internet.\n",
    "- Invented For: Retreival of HTML Pages, Images, Docs, Etc..\n",
    "\n",
    "**Note**: \"URLs\" are part of the \"Set of Rules\" in HTTP Protocol.\n",
    "\n",
    "- Example: \"http://www.google.com\"\n",
    "- **http** : Protocol, **www.google.com** : Host(hostname), -- Next part is Path to Document. \n",
    "\n",
    "### **1.3. Types of Request-!**\n",
    "\n",
    "- One Type of Request is **GET** Request: This is used to **get** the content of page at specified URL. \n",
    "- Server Can Return HTML, or any form of data, such as JSON in case of some specifically designed WEB-APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "Date: Sun, 01 Oct 2023 08:13:37 GMT\n",
      "Server: Apache/2.4.18 (Ubuntu)\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\n",
      "ETag: \"a7-54f6609245537\"\n",
      "Accept-Ranges: bytes\n",
      "Content-Length: 167\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Pragma: no-cache\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Connection: close\n",
      "Content-Type: text/plain\n",
      "\n",
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "# Designing The World's Simplest Web Browser!.\n",
    "# After looking Carefully, we can see -> This is An Client Example!.\n",
    "\n",
    "'''\n",
    "TELNET: Utility used To Connect To A Server And A Port!.\n",
    "$ telnet data.pr4e.org 80\n",
    "\n",
    "Trying 192.241.136.170...\n",
    "Connected to data.pr4e.org.\n",
    "Escape character is '^]'.\n",
    "---------------------------> HERE, I Typed in The Below Line To Get The Data For Page1.htm.\n",
    "GET http://data.pr4e.org/page1.htm HTTP/1.0\n",
    "\n",
    "HTTP/1.1 200 OK\n",
    "Date: Fri, 29 Sep 2023 14:12:00 GMT\n",
    "Server: Apache/2.4.18 (Ubuntu)\n",
    "Last-Modified: Mon, 15 May 2017 11:11:47 GMT\n",
    "ETag: \"80-54f8e1f004857\"\n",
    "Accept-Ranges: bytes\n",
    "Content-Length: 128\n",
    "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
    "Pragma: no-cache\n",
    "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
    "Connection: close\n",
    "Content-Type: text/html\n",
    "\n",
    "<h1>The First Page</h1>\n",
    "<p>\n",
    "If you like, you can switch to the \n",
    "<a href=\"http://data.pr4e.org/page2.htm\">\n",
    "Second Page</a>.\n",
    "</p>\n",
    "Connection closed by foreign host.\n",
    "\n",
    "'''\n",
    "import socket\n",
    "\n",
    "mySock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mySock.connect(('data.pr4e.org', 80))\n",
    "\n",
    "\"\"\"\n",
    "@Reason:    The '\\r\\n\\r\\n' characters are appended to the end of the request header. \n",
    "            This sequence of characters indicates the end of the header section and \n",
    "            the start of the optional body section (which is empty in this case). The first\n",
    "            '\\r\\n' represents a carriage return followed by a line feed character, and the second \n",
    "            '\\r\\n' does the same.\n",
    "\n",
    "@InSummary: In summary, the two newline characters '\\r\\n\\r\\n' are a standard way to \n",
    "            indicate the end of the HTTP request header and are necessary for\n",
    "            sproper communication between the client and server when making HTTP requests.\n",
    "\"\"\"\n",
    "cmd = \"GET http://data.pr4e.org/romeo.txt HTTP/1.0\\r\\n\\r\\n\".encode()\n",
    "mySock.send(cmd)\n",
    "# This Command Sends The Get Request To The URL.\n",
    "\n",
    "while(True):\n",
    "    data = mySock.recv(512)\n",
    "    if (len(data) < 1):\n",
    "        break\n",
    "    print(data.decode(), end='')\n",
    "\n",
    "mySock.close()\n",
    "\n",
    "del(cmd)\n",
    "del(data)\n",
    "del(mySock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "Date: Sun, 01 Oct 2023 08:13:38 GMT\n",
      "Server: Apache/2.4.18 (Ubuntu)\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\n",
      "ETag: \"1d3-54f6609240717\"\n",
      "Accept-Ranges: bytes\n",
      "Content-Length: 467\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Pragma: no-cache\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Connection: close\n",
      "Content-Type: text/plain\n",
      "\n",
      "Why should you learn to write programs?\n",
      "\n",
      "Writing programs (or programming) is a very creative \n",
      "and rewarding activity.  You can write programs for \n",
      "many reasons, ranging from making your living to solving\n",
      "a difficult data analysis problem to having fun to helping\n",
      "someone else solve a problem.  This book assumes that \n",
      "everyone needs to know how to program, and that once \n",
      "you know how to program you will figure out what you want \n",
      "to do with your newfound skills.  \n"
     ]
    }
   ],
   "source": [
    "# Excersise 1:\n",
    "mySock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mySock.connect(('data.pr4e.org', 80))\n",
    "cmd = \"GET http://data.pr4e.org/intro-short.txt HTTP/1.0\\r\\n\\r\\n\".encode()\n",
    "mySock.send(cmd)\n",
    "# This Command Sends The Get Request To The URL.\n",
    "\n",
    "while(True):\n",
    "    data = mySock.recv(512)\n",
    "    if (len(data) < 1):\n",
    "        break\n",
    "    print(data.decode(), end='')\n",
    "\n",
    "mySock.close()\n",
    "\n",
    "del(cmd)\n",
    "del(data)\n",
    "del(mySock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.4. How To Get Images From The Server!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 5120, Counter: 5120\n",
      "Length: 3520, Counter: 8640\n",
      "Length: 5120, Counter: 13760\n",
      "Length: 640, Counter: 14400\n",
      "Length: 5120, Counter: 19520\n",
      "Length: 5120, Counter: 24640\n",
      "Length: 4160, Counter: 28800\n",
      "Length: 5120, Counter: 33920\n",
      "Length: 5120, Counter: 39040\n",
      "Length: 1280, Counter: 40320\n",
      "Length: 2880, Counter: 43200\n",
      "Length: 1440, Counter: 44640\n",
      "Length: 1440, Counter: 46080\n",
      "Length: 2880, Counter: 48960\n",
      "Length: 5120, Counter: 54080\n",
      "Length: 5120, Counter: 59200\n",
      "Length: 5120, Counter: 64320\n",
      "Length: 5120, Counter: 69440\n",
      "Length: 5120, Counter: 74560\n",
      "Length: 5120, Counter: 79680\n",
      "Length: 5120, Counter: 84800\n",
      "Length: 5120, Counter: 89920\n",
      "Length: 5120, Counter: 95040\n",
      "Length: 5120, Counter: 100160\n",
      "Length: 640, Counter: 100800\n",
      "Length: 1440, Counter: 102240\n",
      "Length: 1440, Counter: 103680\n",
      "Length: 5120, Counter: 108800\n",
      "Length: 5120, Counter: 113920\n",
      "Length: 5120, Counter: 119040\n",
      "Length: 1920, Counter: 120960\n",
      "Length: 5120, Counter: 126080\n",
      "Length: 5120, Counter: 131200\n",
      "Length: 5120, Counter: 136320\n",
      "Length: 5120, Counter: 141440\n",
      "Length: 5120, Counter: 146560\n",
      "Length: 5120, Counter: 151680\n",
      "Length: 5120, Counter: 156800\n",
      "Length: 5120, Counter: 161920\n",
      "Length: 2240, Counter: 164160\n",
      "Length: 1440, Counter: 165600\n",
      "Length: 5120, Counter: 170720\n",
      "Length: 5120, Counter: 175840\n",
      "Length: 5120, Counter: 180960\n",
      "Length: 5120, Counter: 186080\n",
      "Length: 5120, Counter: 191200\n",
      "Length: 5120, Counter: 196320\n",
      "Length: 2400, Counter: 198720\n",
      "Length: 5120, Counter: 203840\n",
      "Length: 5120, Counter: 208960\n",
      "Length: 4160, Counter: 213120\n",
      "Length: 1440, Counter: 214560\n",
      "Length: 5120, Counter: 219680\n",
      "Length: 5120, Counter: 224800\n",
      "Length: 5120, Counter: 229920\n",
      "Length: 688, Counter: 230608\n",
      "The Header Length: 394.\n",
      "Header Received: \n",
      "HTTP/1.1 200 OK\n",
      "Date: Sun, 01 Oct 2023 08:13:38 GMT\n",
      "Server: Apache/2.4.18 (Ubuntu)\n",
      "Last-Modified: Mon, 15 May 2017 12:27:40 GMT\n",
      "ETag: \"38342-54f8f2e5b6277\"\n",
      "Accept-Ranges: bytes\n",
      "Content-Length: 230210\n",
      "Vary: Accept-Encoding\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Pragma: no-cache\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Connection: close\n",
      "Content-Type: image/jpeg\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "HOST = 'data.pr4e.org'\n",
    "PORT = 80\n",
    "sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "sock.connect((HOST, PORT))\n",
    "sock.sendall(b\"GET http://data.pr4e.org/cover3.jpg HTTP/1.0\\r\\n\\r\\n\")\n",
    "\n",
    "pictureRecv = b\"\"\n",
    "count = 0\n",
    "\n",
    "while True:\n",
    "    data = sock.recv(5120)\n",
    "    if len(data) < 1: break \n",
    "\n",
    "    count += len(data)\n",
    "    print(f\"Length: {len(data)}, Counter: {count}\")\n",
    "    pictureRecv += data\n",
    "\n",
    "sock.close()\n",
    "\n",
    "\n",
    "# Look For The End of The header!.\n",
    "pos = pictureRecv.find(b\"\\r\\n\\r\\n\")\n",
    "print(f\"The Header Length: {pos}.\")\n",
    "print(f\"Header Received: \\n{pictureRecv[:pos].decode()}\")\n",
    "\n",
    "# Skip: \n",
    "pictureRecv = pictureRecv[(pos + 4):]\n",
    "fd = open(\"../Samples/retreivedImages/retrv1.jpg\", \"wb\")\n",
    "fd.write(pictureRecv)\n",
    "fd.close()\n",
    "\n",
    "del(fd)\n",
    "del(pos)\n",
    "del(count)\n",
    "del(pictureRecv)\n",
    "del(HOST)\n",
    "del(PORT)\n",
    "del(sock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.5. Retrieving web pages with \"urllib\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "# How To Handle \"urlLib\"\n",
    "\n",
    "import urllib.request\n",
    "fd = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "# The Above Line simply opens the \"URL\" then we, can use it however we wish to.\n",
    "for ln in fd:\n",
    "    print(f\"{ln.decode().strip()}\")\n",
    "\n",
    "del(fd)\n",
    "del(ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'But': 1, 'soft': 1, 'what': 1, 'light': 1, 'through': 1, 'yonder': 1, 'window': 1, 'breaks': 1, 'It': 1, 'is': 3, 'the': 3, 'east': 1, 'and': 3, 'Juliet': 1, 'sun': 2, 'Arise': 1, 'fair': 1, 'kill': 1, 'envious': 1, 'moon': 1, 'Who': 1, 'already': 1, 'sick': 1, 'pale': 1, 'with': 1, 'grief': 1}\n"
     ]
    }
   ],
   "source": [
    "# How To Read Binary Files using URL-Lib!.  \n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "# How to open a URL. URL is just like a File!.\n",
    "fd = urllib.request.urlopen(\"http://data.pr4e.org/romeo.txt\")\n",
    "counts = dict()\n",
    "\n",
    "for ln in fd:\n",
    "    # Decode The Byte Array First!. \n",
    "    words = ln.decode().split()\n",
    "    \n",
    "    for singlWord in words:\n",
    "        counts[singlWord] = counts.get(singlWord, 0) + 1\n",
    "\n",
    "print(counts)\n",
    "\n",
    "del(fd)\n",
    "del(ln)\n",
    "del(words)\n",
    "del(counts)\n",
    "del(singlWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Advanced Networking Concepts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASCII Equivalent of \"h\" is 104.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1. Undertanding Concept related To ASCII Values and Byte Arrays.\n",
    "'''\n",
    "# Convert ASCII Character \"H\" to it's ASCII Numeric Value.\n",
    "print(f\"ASCII Equivalent of \\\"h\\\" is {ord('h')}.\")\n",
    "\n",
    "# Different Multi-Byte Characters!:\n",
    "# -> UTF-16: 2 Bytes. \n",
    "# -> UTF-32: 4 Bytes.\n",
    "# -> UTF-8 : 1-4 Bytes.\n",
    "# In Python All String Are: Unicode!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>The First Page</h1>\n",
      "<p>\n",
      "If you like, you can switch to the\n",
      "<a href=\"http://www.dr-chuck.com/page2.htm\">\n",
      "Regex Extracted Link!: [b'href=\"http://www.dr-chuck.com/page2.htm\"']\n",
      "Second Page</a>.\n",
      "</p>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@brief: Read A HTML Page -> Using URLLib.\n",
    "'''\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import re\n",
    "\n",
    "fd = urllib.request.urlopen('http://www.dr-chuck.com/page1.htm')\n",
    "for ln in fd:\n",
    "    print(ln.decode().strip())\n",
    "    regx = re.findall(b'href=\"http[s]?://.*\"', ln)\n",
    "    # ? - Makes it Ungreedy!.  (0 or 1) \n",
    "    # + - Makes it bit Greedy! (1 or More)s\n",
    "    # . - Makes it Greedy!.    (0 or More)\n",
    "    if (len(regx) > 0):\n",
    "        print(f\"Regex Extracted Link!: {regx}\")\n",
    "        \n",
    "del(fd)\n",
    "del(ln)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Note**: \n",
    "- Urllib capability in Python is to **\"scrape\"** the web. \n",
    "- Web scraping is when we write a program that **\"pretends to be a web browser\"** and retrieves pages.\n",
    "- Then, examines the **data in those pages** looking for patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Web-Scraping in Python!**\n",
    "\n",
    "Q. Why Scrape ? \\\n",
    "A. \n",
    "- Pull data - Particularly Social Data - Who Links to Who ? \n",
    "- Get Your data from some system with no-export capability.\n",
    "- Monitor a Site For New Information!.\n",
    "- Spider The Web to make a database for a search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link: 'https://www.iana.org/domains/example'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ssl\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url_dat = str(input(\"Enter URL: \"))\n",
    "html = urllib.request.urlopen(url=url_dat, context=ctx).read()\n",
    "links = re.findall(b'href=\"(http[s]?://.*?)\"', html)\n",
    "for link in links:\n",
    "    print(f\"Link: '{link.decode()}'\")\n",
    "\n",
    "del(ctx)\n",
    "del(link)\n",
    "del(html)\n",
    "del(links)\n",
    "del(url_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag: \"<a href=\"http://www.dr-chuck.com/page2.htm\">\n",
      "Second Page</a>\"\n",
      "\"\n",
      "Second Page\": \"http://www.dr-chuck.com/page2.htm\", Attr: \"({'href': 'http://www.dr-chuck.com/page2.htm'})\"\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import ssl\n",
    "\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "# url = str(input(\"Enter A URL:\"))\n",
    "url = \"http://www.dr-chuck.com/page1.htm\"\n",
    "html = urllib.request.urlopen(url=url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "'''\n",
    "print(f\"Soup Returns Title: \\\"{soup.title}\\\"\")\n",
    "print(f\"Soup Returns Title Name: \\\"{soup.title.name}\\\"\")\n",
    "print(f\"Soup Returns Title String: \\\"{soup.title.string}\\\"\")\n",
    "print(f\"Soup Returns Title Parent Name: \\\"{soup.title.parent.name}\\\"\")\n",
    "print(f\"Soup Returns <P> \\\"{soup.p}\\\"\")\n",
    "'''\n",
    "# Retreive All of Anchor Tags ->\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    print(f\"Tag: \\\"{tag}\\\"\")\n",
    "    print(f\"\\\"{tag.contents[0]}\\\": \\\"{tag.get('href', None)}\\\", Attr: \\\"({tag.attrs})\\\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
